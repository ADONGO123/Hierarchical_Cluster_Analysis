---
title: "Unsupervised Learning: Hierarchical Cluster Analysis"
author: Robert Adongo  
date: "July 25, 2025"
output:
  # cleanrmd::html_document_clean:
  #   theme: NULL
  #   toc: true
  #   toc_float: true
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
    theme: flatly
    highlight: tango
    df_print: paged
    #css: "custom.css"
bibliography: references.bib
csl: apa.csl
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
## Motivation and Goals 
Clustering is a fundamental technique in unsupervised learning, aimed at grouping similar observations based on their characteristics. It is especially useful when no predefined class labels exist and a researcher seeks to uncover hidden structures in the data [@kaufman2009finding].

In this project, we implement the `agnes` function and the `diana` function from the `cluster` package in R to perform agglomerative and divisive hierarchical clustering, respectively. Agglomerative methods start with each observation in its own cluster and iteratively merge the most similar clusters, while divisive methods begin with all observations in a single cluster and recursively split them based on dissimilarity [@kaufman2009finding; @macnaughton1965some].

The goal of this section is to provide a theoretical background for hierarchical clustering, apply both methods to our dataset, and compare the resulting cluster structures using dendrograms and cluster evaluation metrics.

## The `agnes` Function
<pre> agnes(x, diss = inherits(x, "dist"), metric = "euclidean",
      stand = FALSE, method = "average", par.method,
      keep.diss = n < 100, keep.data = !diss, trace.lev = 0) </pre>

## The `diana` Function
<pre> diana(x, diss = inherits(x, "dist"), metric = "euclidean", stand = FALSE,
      stop.at.k = FALSE,
      keep.diss = n < 100, keep.data = !diss, trace.lev = 0)</pre>

# Mathematical Setup
## Dissimilarity and Proximity Matrices
The mathematical setup of this section is based on the book [@hastie2009elements].
In clustering, we often represent how alike or different objects are using a **proximity matrix**. Let $N$ be the number of objects. A proximity matrix $D$ of size $N \times N$ has entries $d_{ii'}$ that reflect dissimilarity between objects $i$ and $i'$. 

Often it is assumed that the following conditions hold:

- **Non-negativity**:  $d_{ii'} \geq 0 \quad \text{for all } i, i' \in \{1,2,\dots, N\}.$

- **Zero diagonal** (self-similarity): $d_{ii} = 0 \quad \text{for all } i = 1, 2, \dots, N.$

- **Symmetry**:  $d_{ii'} = d_{i'i} \quad \text{for all } i, i' \in \{1,2,\dots, N\}.$

If the proximity was originally measured as **similarity** ($s_{ii'}$), it can be converted into dissimilarity using a **monotone-decreasing function**, such as:

- Reciprocal transformation: $d_{ii'} = \frac{1}{s_{ii'}}.$

- Difference from maximum:  $d_{ii'} = \max(S) - s_{ii'}.$

Here, the $S$ is the similarity matrix and $s_{ii'}$ are the entries.
If the matrix is not symmetric, it can be symmetrized using:
$D^{\prime} \leftarrow \frac{D + D^T}{2}.$

Finally, note that subjectively judged dissimilarities often do **not** satisfy the triangle inequality: $d_{ii'} \leq d_{ik} + d_{i'k}, \quad \text{for all } k = 1, \dots, N.$

### Attribute-Based Dissimilarities

In most clustering tasks, we are given data $\mathbf{x}_i = (x_{i1}, x_{i2}, \dots, x_{ip})$ for objects $i = 1, 2, \dots, N$ on $p$ variables (or **attributes**). Since clustering algorithms typically require a **dissimilarity matrix** as input, we first need to define **pairwise dissimilarities** between observations.

We begin by specifying a univariate dissimilarity function $d_j(x_{ij}, x_{i'j})$ for each attribute $j$. A simple (unweighted) measure of dissimilarity between two observations is then:

$$
D(\mathbf{x}_i, \mathbf{x}_{i'}) = \sum_{j=1}^{p} d_j(x_{ij}, x_{i'j}). \quad \text{(1)}
$$

#### Quantitative Variables

For real-valued variables, dissimilarity is typically a function of the absolute difference between values. A common choice is:

* **Squared error**:

  $$
  d_j(x_{ij}, x_{i'j}) = (x_{ij} - x_{i'j})^2. \quad \text{(2)}
  $$

* **Absolute error**:

  $$
  d_j(x_{ij}, x_{i'j}) = |x_{ij} - x_{i'j}|. \quad \text{(3)}
  $$

Alternatively, dissimilarity may be based on **correlation** between attribute profiles of two observations:

$$
\rho(\mathbf{x}_i, \mathbf{x}_{i'}) = \frac{\sum_{j=1}^p (x_{ij} - \bar{x}_i)(x_{i'j} - \bar{x}_{i'})}{\sqrt{\sum_{j=1}^p (x_{ij} - \bar{x}_i)^2} \cdot \sqrt{\sum_{j=1}^p (x_{i'j} - \bar{x}_{i'})^2}}. \quad \text{(4)}
$$

Correlation coefficients—whether parametric (like Pearson) or nonparametric (such as Spearman or Kendall)—can be transformed into dissimilarity measures $d(\mathbf{x}_i, \mathbf{x}_{i'})$. One common approach is:

$$
d(\mathbf{x}_i, \mathbf{x}_{i'}) = \frac{1 - \rho(\mathbf{x}_i, \mathbf{x}_{i'})}{2} \quad \text{(5)}
$$

An alternative method defines dissimilarity based on the absolute value of the correlation:

$$
d(\mathbf{x}_i, \mathbf{x}_{i'}) = 1 - |\rho(\mathbf{x}_i, \mathbf{x}_{i'})| \quad \text{(6)}
$$

For a detailed comparison of these two approaches, see [@lance1979inver], who found that method (5) consistently outperformed the alternative. However, method (6) still demonstrated reasonably strong performance across a variety of datasets.


#### Ordinal Variables

For variables with ordered but not numeric levels (e.g., preference levels or grades), the values can be converted to equally spaced numeric scores. If an ordinal variable has $M$ ordered levels, we map each level $i$ to:

$$
z_i = \frac{i - 0.5}{M} \quad \text{for } i = 1, 2, \dots, M. \quad \text{(7)}
$$

The converted values $z_i$ are then treated as quantitative.

#### Categorical Variables

For nominal variables with $M$ unordered categories, dissimilarity is defined using a symmetric **loss matrix** $L$ with entries:

* $L_{rr'} = 0$ if $r = r'$
* $L_{rr'} \geq 0$ for $r \ne r'.$

A common choice is:

$$
L_{rr'} = 
\begin{cases}
0 & \text{if } r = r' \\
1 & \text{if } r \ne r'.
\end{cases}
$$

This treats all mismatches as equally dissimilar, but $L$ can be customized to weight some mismatches more heavily than others.


### Weighted Dissimilarities

We often want to control how much each attribute contributes to the overall dissimilarity. To do this, we assign **weights** $w_j$ to attributes and compute:

$$
D(\mathbf{x}_i, \mathbf{x}_{i'}) = \sum_{j=1}^{p} w_j \cdot d_j(x_{ij}, x_{i'j}). \quad \text{(8)}
$$

with $\sum_{j=1}^{p} w_j = 1.$

However, assigning equal weights ($w_j = 1/p$) does **not necessarily** ensure equal influence from all variables. A variable's true influence depends on its **average pairwise dissimilarity**, defined as:

$$
\bar{d}_j = \frac{1}{N^2} \sum_{i=1}^{N} \sum_{i'=1}^{N} d_j(x_{ij}, x_{i'j}). \quad \text{(10)}
$$

To equalize influence, weights can be set **inversely proportional** to $\bar{d}_j.$ That is, 
$w_j \propto \frac{1}{\bar{d}_j}.$

Suppose all variables are quantitative and squared differences are used, 

$$
D(\mathbf{x}_i, \mathbf{x}_{i'}) = \sum_{j=1}^p w_j (x_{ij} - x_{i'j})^2. \quad \text{(11)}
$$

This is the **(weighted) squared Euclidean distance**. In this case;

$$
\bar{d}_j = \frac{1}{N^2} \sum_{i=1}^{N} \sum_{i'=1}^{N} (x_{ij} - x_{i'j})^2 = 2 \cdot \text{Var}(X_j). \quad \text{(12)}
$$

So variables with higher variance will have more influence unless weights are adjusted.

## Clustering Algorithms
There are numerous clustering algorithms in the literature, making it impractical to review them all. [@bock1974automatische] conducted a comprehensive survey reflecting the state of the field in the early 1970s, but the area has grown significantly since then. This report focuses on hierarchical clustering, specifically agglomerative and divisive methods. Unlike other methods, hierarchical clustering does not require pre-specifying the number of clusters. Instead, it relies on a dissimilarity measure between groups, built from pairwise dissimilarities. The algorithm builds a hierarchy: starting with each observation as its own cluster, and successively merging them until all data points form a single cluster.


### Agglomerative Clustering (Nesting)

Agglomerative clustering starts by treating each observation \( \mathbf{x}_i \) as its own cluster. At each of the \( N - 1 \) steps, the two least dissimilar clusters are merged, reducing the total number of clusters by one. To perform this merging, we define the dissimilarity between two groups of observations or clusters, \( G \) and \( H \), using the pairwise dissimilarities \( d(\mathbf{x}_i, \mathbf{x}_{i'}) \) for \( i \in G \), \( i' \in H \). Let $|G|$ and $|H|$ denote their number of objects.

Below are three commonly used definitions of intergroup dissimilarity:

- **Single Linkage (Nearest Neighbor)** introduced by [@florek1951liaison;@sneath1957application]:
\[
d_{SL}(G, H) = \min_{i \in G, i' \in H} d(\mathbf{x}_i, \mathbf{x}_{i'})
\]

- **Complete Linkage (Furthest Neighbor)** described by [@mcquitty1960hierarchical;@sokal1963principles;@macnaughton1965some]:
\[
d_{CL}(G, H) = \max_{i \in G, i' \in H} d(\mathbf{x}_i, \mathbf{x}_{i'})
\]

- **Group Average** by [@sokal1958statistical]:
\[
d_{GA}(G, H) = \frac{1}{|G||H|} \sum_{i \in G} \sum_{i' \in H} d(\mathbf{x}_i, \mathbf{x}_{i'})
\]

Statistically, only the group average method estimates a meaningful population-level quantity as the sample size increases. As  $N \to \infty$,  the group average dissimilarity  $d_{GA}(G, H)$  converges to the expected dissimilarity between two distributions $p_G(x)$ and $p_H(x')$:

$$
\int d(x, x') \, p_G(x) \, p_H(x') \, dx \, dx'
$$

In contrast, single linkage and complete linkage do not estimate any meaningful population property in the limit. Single linkage tends toward zero regardless of the underlying distributions, and complete linkage tends toward infinity.

### Divisive Clustering

Divisive clustering is a top-down hierarchical method. It starts with the full dataset as one cluster and recursively splits it into two subclusters at each step. Although less studied than agglomerative methods, divisive clustering can be advantageous when the goal is to find a small number of large, well-separated clusters [@hastie2009elements].

A simple but effective algorithm was proposed by [@macnaughton1965dissimilarity]. It begins by placing all observations in a single cluster \( G \). The algorithm selects the observation with the highest average dissimilarity from the rest to form a new cluster \( H \). At each step, it moves the observation in \( G \) that is most distant—on average—from other members of \( G \) and closest to \( H \). The process stops when no remaining observation in \( G \) is, on average, closer to those in \( H \).

This results in a split: the observations in \( H \) and those remaining in \( G \). These become the two daughter clusters at the next level of the hierarchy.

The process continues recursively. At each level, the cluster to be split is often chosen based on having the largest diameter:

$$
\text{diam}(G) = \max_{i,j \in G} d_{ij}
$$

An alternative is to choose the cluster with the largest average dissimilarity among its members:

$$
\bar{d}_G = \frac{1}{|G|^2} \sum_{i \in G} \sum_{i' \in G} d_{i i'}
$$

Splitting continues until all clusters are either singletons or all members within each cluster are identical (i.e., have zero dissimilarity).

Unlike divisive methods based on k-means or k-medoids with \( K = 2 \), this approach does not depend on initialization and preserves the monotonicity needed for dendrogram representation.

# Applications
```{r loading libraries and data,message=FALSE, warning=FALSE}
# Loading Libraries
 library(tidyverse)
 library(ggrepel)
 library(cluster)
 library(factoextra)
 library(psych)
library(gridExtra)
#Reading Dataset
dailyActivity<- read_csv("Datasets/dailyActivity_merged.csv", show_col_types = FALSE)
dailyActivity <- dailyActivity[,c(3,4,5,15)]
```

In this report, we will only look at continuous variables from the  **FitBit Fitness Tracker Data**. This dataset includes \( P = 13 \) variables measured across \( n = 940 \) observations from wearable fitness devices. The data combines daily activity records, sleep duration, hourly intensity levels, and second-level heart rate readings. It was obtained from Kaggle and constructed by merging four selected files from the original FitBit collection. For more details on this dataset, see **[FitBit Fitness Tracker Data](https://www.kaggle.com/datasets/arashnic/fitbit/data)**.


**Dissimilarity Matrix**

We start by defining a matrix of dissimilarity using the eucledian distance. As the variables have different magnitude of measurement, we will carry a z-score standardization. 

```{r, 2}
# Summary Statistics
describe(dailyActivity)

# Dissimilarity matrix
D_matrix<-dist(x=scale(dailyActivity),method="euclidean")
as.matrix(D_matrix)[3,9]
```
The dissimilarity matrix `D_matrix` contains the pairwise Euclidean distances between 940 observations from the daily activity data. Each value reflects how similar or different two observations are based on the four standardized variables. For instance, the distance between Observation 3 and Observation 9 is approximately 0.031, indicating that these two users have very similar activity patterns across all measured features.

With the dissimilarity matrix computed, we can now perform clustering. We'll begin with agglomerative clustering using just the group average method, as described in Section 2.2.1.

```{r}
# Group Average
cluster_agg_average <- agnes(x = D_matrix, method = "average")

# Single Linkage
#cluster_agg_single <- agnes(x = D_matrix, method = "single")

# Complete Linkage
#cluster_agg_complete <- agnes(x = D_matrix, method = "complete")

```

**Average Linkage Method**

We represent the results of the agglomerative clustering using the average linkage method by extracting the merging stages and corresponding distances from the `cluster_agg_average` object. Specifically, we combine the `merge` matrix and the `height` vector into a single data frame.

```{r}
coefficients_agg_average <- cluster_agg_average$height

 agg_average <- as.data.frame(cbind(Stage=1:length(coefficients_agg_average), cluster_agg_average$merge,coefficients_agg_average))

names(agg_average)<- c("Stage", "Cluster 1", "Cluster 2", "Distance")
head(agg_average)
```

The `agg_average` data frame presents the agglomeration schedule obtained from hierarchical clustering using the **average linkage** method. At each stage, the two clusters or individual observations with the smallest average pairwise Euclidean distance between their members are merged, as reflected in the `Distance` column. The negative values under `Cluster 1` and `Cluster 2` denote raw observations, while positive values refer to clusters formed at earlier stages.

```{r, message=FALSE, warning=FALSE}
average_dendo <- fviz_dend(cluster_agg_average,k = 2,
                          k_colors = c("orange", "darkorchid"),
                          color_labels_by_k = FALSE,
                          rect = TRUE,
                          rect_fill = TRUE,
                          lwd = 1,
                          ggtheme = theme_bw()) +
                          ggtitle("Average Linkage")
                          
average_dendo
```
To reinforce the analysis, we will study the dendrogram above. The dendrogram shows clear groupings among the 940 individuals based on their activity levels. Most individuals are merged at low heights, indicating many users share similar fitness behaviors. A large vertical jump near the top suggests the presence of 2–3 distinct clusters, possibly reflecting differences between highly active, moderately active, and sedentary users. The use of average linkage ensures these clusters are formed based on overall similarity rather than extremes.

Next, we will do a Silhouette Analysis to see the appropriate number of  clusters to use further for our analysis. This evaluates how similar each object is to its own cluster compared to other clusters. A higher average silhouette width indicates better-defined clusters. We will use the `silhouette()` function in the `cluster` package to do this

```{r}
sil_widths <- sapply(2:10, function(k){
  cut_clusters <- cutree(cluster_agg_average, k = k)
  silhouette_info <- silhouette(cut_clusters, dist(D_matrix))
  mean(silhouette_info[, 3])
})

# Plot 
plot(2:10, sil_widths, type = "b", xlab = "Number of clusters", ylab = "Average silhouette width")

best_k <- which.max(sil_widths) + 1
best_k
```
After selecting a solution with two clusters, the `cutree()` function in R can be used to determine the cluster membership of each observation. This function requires two main arguments: the hierarchical clustering object (passed through the `tree` parameter) and the desired number of clusters, specified using the `k` argument.

```{r}
dailyActivity["H_cluster"] <- cutree(tree = cluster_agg_average, k = 2)
table(dailyActivity$H_cluster)
```
At this stage, the hierarchical agglomerative clustering process can be viewed as complete. However, by creating a new variable such as `H_cluster`, we can go further by applying a one-way ANOVA test. This allows for the examination of whether a specific variable shows significant differences across the formed clusters—essentially testing whether the variation between clusters is greater than the variation within them.

```{r}
#Total steps
aov_totalSteps <- aov(TotalSteps~H_cluster, data = dailyActivity)
summary(aov_totalSteps)

#Total Distance
aov_totalDistance <- aov(TotalDistance~H_cluster, data = dailyActivity)
summary(aov_totalDistance)

# Tracker Distance
aov_trackerDistance <- aov(TrackerDistance~H_cluster, data = dailyActivity)
summary(aov_trackerDistance)

# Calories
aov_Calories <- aov(Calories~H_cluster, data = dailyActivity)
summary(aov_Calories)
```
The ANOVA tests show statistically significant differences in **Total Steps**, **Total Distance**, **Tracker Distance**, and **Calories burned** between the two clusters identified through hierarchical clustering (`p < 0.001` in all cases). This indicates that the clusters differ meaningfully in terms of overall physical activity. Despite one cluster containing only 3 observations, these differences are strong enough to produce high F-values, and this is because the small cluster likely represents outliers with extreme activity patterns. 

```{r}
dailyActivity[dailyActivity$H_cluster == 2, ]
```
When we compare the observations from the smaller cluster (2) above to the summary statistics in Chunk 2, we see that these observations are likely outliers.

**Divisive Method**

We use the `diana` function to compute divisive clustering. To determine the number of clusters to use, we use the Silhouette Analysis. The results show that three clusters is appropriate.

```{r}
div_average <- diana(D_matrix, stand=T)

sil_widths_div <- sapply(2:10, function(k){
  cut_clusters <- cutree(div_average, k = k)
  silhouette_info <- silhouette(cut_clusters, dist(D_matrix))
  mean(silhouette_info[, 3])
})

# Plot 
plot(2:10, sil_widths_div, type = "b", xlab = "Number of clusters", ylab = "Average silhouette width")

best_k <- which.max(sil_widths_div) + 1
best_k
```
Now, we visualize the output of `DIANA`  as dendrograms using the function `fviz_dend()` from the `factoextra` package.

```{r}
fviz_dend(div_average, cex = 0.5,
          k = 3, # Cut in four groups
          palette = "jco" # Color palette
          )
```

```{r}
dailyActivity["H_cluster_div"] <- cutree(tree = div_average, k = 3)
table(dailyActivity$H_cluster_div)
```
```{r}
#Total steps
aov_totalSteps.div <- aov(TotalSteps~H_cluster_div, data = dailyActivity)
summary(aov_totalSteps.div)

#Total Distance
aov_totalDistance.div <- aov(TotalDistance~H_cluster_div, data = dailyActivity)
summary(aov_totalDistance.div)

# Tracker Distance
aov_trackerDistance.div <- aov(TrackerDistance~H_cluster_div, data = dailyActivity)
summary(aov_trackerDistance.div)

# Calories
aov_Calories.div <- aov(Calories~H_cluster_div, data = dailyActivity)
summary(aov_Calories.div)
```
**Conclusion**

The Agglomerative clustering identified only a few strong outliers, while divisive clustering captured a more detailed and structured grouping of the data.

# References